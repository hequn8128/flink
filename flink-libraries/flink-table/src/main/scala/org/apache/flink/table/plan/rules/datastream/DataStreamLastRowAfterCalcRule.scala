/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.flink.table.plan.rules.datastream

import org.apache.calcite.plan.RelOptRule.{any, operand}
import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall, RelTraitSet}
import org.apache.calcite.rel.RelNode
import org.apache.calcite.rex.{RexCall, RexInputRef}
import org.apache.calcite.sql.SqlKind
import org.apache.flink.table.calcite.FlinkTypeFactory
import org.apache.flink.table.plan.nodes.FlinkConventions
import org.apache.flink.table.plan.nodes.datastream.{DataStreamLastRow, UpsertStreamScan}
import org.apache.flink.table.plan.nodes.logical.{FlinkLogicalCalc, FlinkLogicalNativeTableScan}
import org.apache.flink.table.plan.schema.{RowSchema, UpsertStreamTable}

import scala.collection.JavaConversions._

/**
  * Use this rule to generate LastRow after Calc to decrease state size in LastRow. The cost of plan
  * will be smaller comparing to the plan generated by [[DataStreamLastRowRule]].
  */
class DataStreamLastRowAfterCalcRule
  extends RelOptRule(
    operand(classOf[FlinkLogicalCalc],
      operand(classOf[FlinkLogicalNativeTableScan], any())),
    "DataStreamLastRowAfterCalcRule") {

  override def matches(call: RelOptRuleCall): Boolean = {
    val calc = call.rel(0).asInstanceOf[FlinkLogicalCalc]
    val scan = call.rel(1).asInstanceOf[FlinkLogicalNativeTableScan]
    val upsertStreamTable = scan.getTable.unwrap(classOf[UpsertStreamTable[Any]])

    // is upsert source
    upsertStreamTable.isInstanceOf[UpsertStreamTable[Any]] &&
    // column pruning or push Filter down
      calc.getRowType.getFieldCount <= scan.getRowType.getFieldCount &&
    // key field should not be changed
      containKeysAfterCalc(upsertStreamTable.uniqueKeys, calc) &&
    // row time field should not be pruning. We plan to use row time filed to insure records order.
      countRowTimeField(calc) == countRowTimeField(scan)
  }

  override def onMatch(call: RelOptRuleCall): Unit = {
    val calc = call.rel(0).asInstanceOf[FlinkLogicalCalc]
    val scan = call.rel(1).asInstanceOf[FlinkLogicalNativeTableScan]

    val traitSet: RelTraitSet = scan.getTraitSet.replace(FlinkConventions.DATASTREAM)
    val scanSchema = new RowSchema(scan.getRowType)
    val calcSchema = new RowSchema(calc.getRowType)

    val upsertStreamScan = new UpsertStreamScan(
      scan.getCluster, traitSet, scan.getTable, scanSchema)
    val newCalc = calc.copy(calc.getTraitSet, upsertStreamScan, calc.getProgram)
    val convInput: RelNode = RelOptRule.convert(newCalc, FlinkConventions.DATASTREAM)

    // get unique key indexes
    val oldKeyNames = scan.getTable.unwrap(classOf[UpsertStreamTable[_]]).uniqueKeys
    val newKeyNames = getUnqueKeysAfterCalc(oldKeyNames, calc)
    val newKeyIndexes = newCalc.getRowType.getFieldNames.zipWithIndex
      .filter(e => newKeyNames.contains(e._1))
      .map(_._2)

    val dataStreamLastRow = new DataStreamLastRow(
      scan.getCluster,
      traitSet,
      convInput,
      calcSchema,
      calcSchema,
      newKeyIndexes)
    call.transformTo(dataStreamLastRow)
  }

  def getInOutNames(calc: FlinkLogicalCalc): Seq[(String, String)] = {
    val inNames = calc.getInput.getRowType.getFieldNames
    calc.getProgram.getNamedProjects
      .map(p => {
        calc.getProgram.expandLocalRef(p.left) match {
          // output field is forwarded input field
          case r: RexInputRef => (r.getIndex, p.right)
          // output field is renamed input field
          case a: RexCall if a.getKind.equals(SqlKind.AS) =>
            a.getOperands.get(0) match {
              case ref: RexInputRef =>
                (ref.getIndex, p.right)
              case _ =>
                (-1, p.right)
            }
          // output field is not forwarded from input
          case _ => (-1, p.right)
        }
      })
      .filter(_._1 >= 0)
      .map(io => (inNames.get(io._1), io._2))
  }

  def containKeysAfterCalc(keys: Seq[String], calc: FlinkLogicalCalc): Boolean = {
    // get input output names
    val inOutNames = getInOutNames(calc)
    // contains all input keys
    inOutNames.map(_._1).containsAll(keys)
  }

  def getUnqueKeysAfterCalc(keys: Array[String], calc: FlinkLogicalCalc): Seq[String] = {
    val inOutNames = getInOutNames(calc)
    inOutNames.filter(e => keys.contains(e._1)).map(_._2)
  }

  def countRowTimeField(rel: RelNode): Int = {
    rel.getRowType.getFieldList.count(e => FlinkTypeFactory.isRowtimeIndicatorType(e.getType))
  }
}

object DataStreamLastRowAfterCalcRule {
  val INSTANCE = new DataStreamLastRowAfterCalcRule
}
